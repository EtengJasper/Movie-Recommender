{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Unsupervised Predict: Explore Data Science Academy\n## Movie Recommender Engine ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:14.954134Z","iopub.execute_input":"2021-06-13T10:37:14.954606Z","iopub.status.idle":"2021-06-13T10:37:14.96648Z","shell.execute_reply.started":"2021-06-13T10:37:14.954502Z","shell.execute_reply":"2021-06-13T10:37:14.965047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importing Libraries","metadata":{}},{"cell_type":"code","source":"# Install packages here\n# Packages for data processing\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport math\nimport random\nimport pickle\nimport json\nimport matplotlib.pyplot as plt\nimport re\nimport warnings\n\nfrom datetime import datetime\nfrom scipy.sparse import csr_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.sparse.linalg import svds\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nfrom wordcloud import WordCloud, STOPWORDS\n\n# !pip install scikit-surprise\nfrom surprise import Reader, Dataset\nfrom surprise.model_selection import cross_validate\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom surprise import NormalPredictor\nfrom surprise import SVDpp, SVD\nfrom surprise import NMF\nfrom surprise import KNNBasic, KNNWithMeans, KNNWithZScore, KNNBaseline\nfrom surprise import BaselineOnly, SlopeOne, CoClustering\nfrom surprise.accuracy import rmse\nfrom surprise import accuracy\nfrom surprise.model_selection import train_test_split\nfrom surprise.model_selection import GridSearchCV\n\nwarnings.filterwarnings(\"ignore\")\nsns.set_style('dark')\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:16.612363Z","iopub.execute_input":"2021-06-13T10:37:16.612872Z","iopub.status.idle":"2021-06-13T10:37:17.925384Z","shell.execute_reply.started":"2021-06-13T10:37:16.612826Z","shell.execute_reply":"2021-06-13T10:37:17.924474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ignore warnings\nimport warnings\nwarnings.simplefilter(action='ignore')\n\n#Install Prerequisites\nimport sys\n#!{sys.executable} -m pip install scikit-learn scikit-surprise\n#!pip install git+https://github.com/gbolmier/funk-svd\n\n# Exploratory Data Analysis\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Data Preprocessing\nimport random\nfrom time import time\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.ticker import NullFormatter\nfrom sklearn.preprocessing import StandardScaler\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n\n# Models\nfrom surprise import Reader, Dataset\nfrom surprise import SVD, NormalPredictor, BaselineOnly, NMF, SlopeOne, CoClustering\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Performance Evaluation\nfrom surprise import accuracy\nfrom sklearn.metrics import mean_squared_error\nfrom surprise.model_selection import GridSearchCV, cross_validate, train_test_split\n\n# Display\n%matplotlib inline\nsns.set(font_scale=1)\nsns.set_style(\"white\")\npd.set_option('display.max_columns', 37)\nfrom wordcloud import WordCloud \nimport warnings\nwarnings.filterwarnings('ignore')\nfrom IPython.display import display_html \nfrom IPython.core.display import HTML\nfrom collections import defaultdict\n#import datetime\nimport re\nimport squarify\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# Visualisation\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nimport plotly.offline as py\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\npy.init_notebook_mode(connected = True)\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:17.927001Z","iopub.execute_input":"2021-06-13T10:37:17.927484Z","iopub.status.idle":"2021-06-13T10:37:19.728684Z","shell.execute_reply.started":"2021-06-13T10:37:17.927451Z","shell.execute_reply":"2021-06-13T10:37:19.727856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reading in Data","metadata":{}},{"cell_type":"code","source":"#Import datasets\nsample_submission = pd.read_csv('/kaggle/input/edsa-movie-recommendation-challenge/sample_submission.csv')\nmovies = pd.read_csv('/kaggle/input/edsa-movie-recommendation-challenge/movies.csv')\nimdb_data = pd.read_csv('/kaggle/input/edsa-movie-recommendation-challenge/imdb_data.csv')\ngenome_scores = pd.read_csv('/kaggle/input/edsa-movie-recommendation-challenge/genome_scores.csv')\ngenome_tags = pd.read_csv('/kaggle/input/edsa-movie-recommendation-challenge/genome_tags.csv')\ntrain = pd.read_csv('/kaggle/input/edsa-movie-recommendation-challenge/train.csv')\ntest = pd.read_csv('/kaggle/input/edsa-movie-recommendation-challenge/test.csv')\ntags = pd.read_csv('/kaggle/input/edsa-movie-recommendation-challenge/tags.csv')\nlinks = pd.read_csv('/kaggle/input/edsa-movie-recommendation-challenge/links.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:19.730424Z","iopub.execute_input":"2021-06-13T10:37:19.730923Z","iopub.status.idle":"2021-06-13T10:37:41.157707Z","shell.execute_reply.started":"2021-06-13T10:37:19.730888Z","shell.execute_reply":"2021-06-13T10:37:41.156838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:41.161967Z","iopub.execute_input":"2021-06-13T10:37:41.163813Z","iopub.status.idle":"2021-06-13T10:37:41.193503Z","shell.execute_reply.started":"2021-06-13T10:37:41.16377Z","shell.execute_reply":"2021-06-13T10:37:41.192417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:41.194634Z","iopub.execute_input":"2021-06-13T10:37:41.194898Z","iopub.status.idle":"2021-06-13T10:37:41.206364Z","shell.execute_reply.started":"2021-06-13T10:37:41.194872Z","shell.execute_reply":"2021-06-13T10:37:41.205298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Preprocessing & Preparation","metadata":{}},{"cell_type":"markdown","source":"Get an indication of the size of the different datasets","metadata":{"execution":{"iopub.status.busy":"2021-06-12T14:07:09.995675Z","iopub.execute_input":"2021-06-12T14:07:09.996068Z","iopub.status.idle":"2021-06-12T14:07:10.0091Z","shell.execute_reply.started":"2021-06-12T14:07:09.996038Z","shell.execute_reply":"2021-06-12T14:07:10.004194Z"}}},{"cell_type":"code","source":"# Create a list consisting of the names of our dataframes\ndfs = [train, test, genome_scores, genome_tags, imdb_data, links, movies, tags]\n# Declare a list of the names of the imported datasets\ndf_names = ['train', 'test', 'genome_scores', 'genome_tags',\n            'imdb_data', 'links', 'movies', 'tags']\ndfs_dict = {}  # declare an empty dictionary in preparation for dataframe view\nfor name, data in zip(df_names, dfs):  # iterate over the zipped lists \n    dfs_dict[name] = [data.shape[0], data.shape[1]] # retrieve row and column sizes\n    # convert dictionary into dataframe\n    df_prop = pd.DataFrame(dfs_dict,\n                          index=['rows', 'columns']).transpose()\ndf_properties = df_prop.sort_values(by='rows', ascending=False) # Arrange dataframes in descending order of samples (rows)\n\ndf_properties  # View shapes of dataframes","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:41.207747Z","iopub.execute_input":"2021-06-13T10:37:41.20805Z","iopub.status.idle":"2021-06-13T10:37:41.236072Z","shell.execute_reply.started":"2021-06-13T10:37:41.208023Z","shell.execute_reply":"2021-06-13T10:37:41.234996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Obtain a feel of nature of data present in train dataset\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:41.237519Z","iopub.execute_input":"2021-06-13T10:37:41.237826Z","iopub.status.idle":"2021-06-13T10:37:41.260022Z","shell.execute_reply.started":"2021-06-13T10:37:41.237796Z","shell.execute_reply":"2021-06-13T10:37:41.258438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Understand what data types are present within our two main datasets (train and test)","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:41.264986Z","iopub.execute_input":"2021-06-13T10:37:41.265532Z","iopub.status.idle":"2021-06-13T10:37:41.287609Z","shell.execute_reply.started":"2021-06-13T10:37:41.265479Z","shell.execute_reply":"2021-06-13T10:37:41.286297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:41.289509Z","iopub.execute_input":"2021-06-13T10:37:41.289935Z","iopub.status.idle":"2021-06-13T10:37:41.313081Z","shell.execute_reply.started":"2021-06-13T10:37:41.28989Z","shell.execute_reply":"2021-06-13T10:37:41.311481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Establish how much data is missing","metadata":{}},{"cell_type":"markdown","source":"train_missing = pd.DataFrame(train.isnull().sum())\ntrain_missing","metadata":{}},{"cell_type":"code","source":"test_missing = pd.DataFrame(test.isnull().sum())\ntest_missing","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:41.314897Z","iopub.execute_input":"2021-06-13T10:37:41.315447Z","iopub.status.idle":"2021-06-13T10:37:41.347813Z","shell.execute_reply.started":"2021-06-13T10:37:41.315392Z","shell.execute_reply":"2021-06-13T10:37:41.346569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g_scores_missing = pd.DataFrame(genome_scores.isnull().sum())\ng_scores_missing","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:41.349869Z","iopub.execute_input":"2021-06-13T10:37:41.350347Z","iopub.status.idle":"2021-06-13T10:37:41.442292Z","shell.execute_reply.started":"2021-06-13T10:37:41.35028Z","shell.execute_reply":"2021-06-13T10:37:41.441211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g_tags_missing = pd.DataFrame(genome_tags.isnull().sum())\ng_tags_missing","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:41.446034Z","iopub.execute_input":"2021-06-13T10:37:41.446397Z","iopub.status.idle":"2021-06-13T10:37:41.458607Z","shell.execute_reply.started":"2021-06-13T10:37:41.446364Z","shell.execute_reply":"2021-06-13T10:37:41.457123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies_missing = pd.DataFrame(movies.isnull().sum())\nmovies_missing","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:41.460275Z","iopub.execute_input":"2021-06-13T10:37:41.460697Z","iopub.status.idle":"2021-06-13T10:37:41.492467Z","shell.execute_reply.started":"2021-06-13T10:37:41.460664Z","shell.execute_reply":"2021-06-13T10:37:41.490406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"links_missing = pd.DataFrame(links.isnull().sum())\nlinks_missing","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:41.494057Z","iopub.execute_input":"2021-06-13T10:37:41.494408Z","iopub.status.idle":"2021-06-13T10:37:41.505305Z","shell.execute_reply.started":"2021-06-13T10:37:41.49436Z","shell.execute_reply":"2021-06-13T10:37:41.504494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tags_missing = pd.DataFrame(tags.isnull().sum())\ntags_missing","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:41.506568Z","iopub.execute_input":"2021-06-13T10:37:41.507031Z","iopub.status.idle":"2021-06-13T10:37:41.623153Z","shell.execute_reply.started":"2021-06-13T10:37:41.506996Z","shell.execute_reply":"2021-06-13T10:37:41.622308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_missing = pd.DataFrame(imdb_data.isnull().sum())\nimdb_missing","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:41.624639Z","iopub.execute_input":"2021-06-13T10:37:41.624947Z","iopub.status.idle":"2021-06-13T10:37:41.643922Z","shell.execute_reply.started":"2021-06-13T10:37:41.624917Z","shell.execute_reply":"2021-06-13T10:37:41.642814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above outputs show that there is no missing data from our primary datasets (train and test). There is a negligible proportion of missing values within the links and tags datasets. A considerable number of missing entries occurs within the imdb dataset which would require further investigation. Let's determine what percentage of data is missing in each of this dataset's columns. ","metadata":{}},{"cell_type":"code","source":"# Calculating the percentage of missing values in each column of the imdb dataset\ntotal = imdb_data.isnull().sum().sort_values(ascending=False)\npercent_1 = imdb_data.isnull().sum()/imdb_data.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2],\n                         axis=1, keys=['Total', '(%) missing'])\nmissing_data['(%) missing'].plot(kind='barh')\nplt.xlabel('(%) Missing Values')\nplt.ylabel('Columns with Missing Values')\nplt.title('Percentage of Missing Values per Column')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:41.647877Z","iopub.execute_input":"2021-06-13T10:37:41.648276Z","iopub.status.idle":"2021-06-13T10:37:41.976159Z","shell.execute_reply.started":"2021-06-13T10:37:41.648237Z","shell.execute_reply":"2021-06-13T10:37:41.975405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is clear from the above chart that there is a large proportion of missing entries across all variables (barring moviedId) with the effect that this precludes the use of this dataset for modeling purposes.","metadata":{}},{"cell_type":"code","source":"# Ascertain how many users and movies there are and whether there are any duplications (within the train dataset)\nusers = len(train.userId.unique())\nitems = len(train.movieId.unique())\nprint('There are {} unique users and {}\\\n unique movies train dataset with {} duplicated entries'.format(users, items, train[train.duplicated()].count().sum()))","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:41.977161Z","iopub.execute_input":"2021-06-13T10:37:41.977786Z","iopub.status.idle":"2021-06-13T10:37:47.290978Z","shell.execute_reply.started":"2021-06-13T10:37:41.977744Z","shell.execute_reply":"2021-06-13T10:37:47.28956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge dataframes into one consolidated dataframe in preparation for more advanced EDA\n# Merge train (users) and movies datasets on movieId column\ntrain_movies_df = pd.merge(train,\n                           movies,\n                           how='left',\n                           on='movieId')\n\n# Perform a further merge with imdb_data on movieId column\nmovies_metadata_df = pd.merge(train_movies_df,\n                              imdb_data,\n                              how='left',\n                              on='movieId')\n\nmovies_metadata_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:47.292671Z","iopub.execute_input":"2021-06-13T10:37:47.293111Z","iopub.status.idle":"2021-06-13T10:37:53.219765Z","shell.execute_reply.started":"2021-06-13T10:37:47.293061Z","shell.execute_reply":"2021-06-13T10:37:53.218395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate average rating for each movie and arrange from highest to lowest\nmovies_ranking = movies_metadata_df[['title','rating']].groupby('title').mean().sort_values('rating', ascending=False)\nmovies_ranking.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:53.221069Z","iopub.execute_input":"2021-06-13T10:37:53.22143Z","iopub.status.idle":"2021-06-13T10:37:57.258414Z","shell.execute_reply.started":"2021-06-13T10:37:53.221402Z","shell.execute_reply":"2021-06-13T10:37:57.257404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Keep in mind that the above average ratings do not take movie popularity into account, in other words the number of people who have watched the movie. We will factor this in later and calculate weighted ratings. ","metadata":{}},{"cell_type":"markdown","source":"### EDA\n\nDiscovery phase and data understanding","metadata":{}},{"cell_type":"markdown","source":"**Most common Genres**","metadata":{}},{"cell_type":"code","source":"\"\"\"\n# Create dataframe containing only the movieId and genres\nmovies_genres = pd.DataFrame(df_movies[['movieId', 'genres']],\n                             columns=['movieId', 'genres'])\n\n# Split genres seperated by \"|\" and create a list containing the genres allocated to each movie\nmovies_genres.genres = movies_genres.genres.apply(lambda x: x.split('|'))\n\n# Create expanded dataframe where each movie-genre combination is in a seperate row\nmovies_genres = pd.DataFrame([(tup.movieId, d) for tup in movies_genres.itertuples() for d in tup.genres],\n                             columns=['movieId', 'genres'])\n\nmovies_genres.head()\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:57.259745Z","iopub.execute_input":"2021-06-13T10:37:57.260138Z","iopub.status.idle":"2021-06-13T10:37:57.266519Z","shell.execute_reply.started":"2021-06-13T10:37:57.260094Z","shell.execute_reply":"2021-06-13T10:37:57.265595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n# Plot the genres from most common to least common\nplot = plt.figure(figsize=(15, 10))\nplt.title('Most common genres\\n', fontsize=20)\nsns.countplot(y=\"genres\", data=movies_genres,\n              order=movies_genres['genres'].value_counts(ascending=False).index,\n              palette='Reds_r')\nplt.show()\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:57.267573Z","iopub.execute_input":"2021-06-13T10:37:57.267847Z","iopub.status.idle":"2021-06-13T10:37:57.282485Z","shell.execute_reply.started":"2021-06-13T10:37:57.26782Z","shell.execute_reply":"2021-06-13T10:37:57.2813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Determine how the ratings are distributed across the rating scale (from 0.5 to 5.0)","metadata":{}},{"cell_type":"code","source":"print (f'Average rating in the dataset: {np.mean(train[\"rating\"])}')\n\nwith sns.axes_style('white'):\n    g = sns.factorplot(\"rating\", data=train, aspect=2.5, kind='count')\n    g.set_ylabels(\"Total number of ratings\")","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:37:57.284065Z","iopub.execute_input":"2021-06-13T10:37:57.284552Z","iopub.status.idle":"2021-06-13T10:38:00.289239Z","shell.execute_reply.started":"2021-06-13T10:37:57.284496Z","shell.execute_reply":"2021-06-13T10:38:00.28837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The mean rating of approx 3.5 shows that users typically lean towards more favourable ratings and are not excessively harsh in their criticism. There are relatively few ratings below 2.0. An alternative explanation could be that users tend to only rate those movies that they enjoy. Many users might terminate an unenjoyable movie mid-stream without bothering to give it a rating. I've personally walked out of a cinema just 15 minutes into the showing of a junk movie, figuring I can make better use of my valuable time and simply deeming the purchase of my movie ticket as a sunk cost. The mode ranking in the chart above is 4.0. It appears that users tend to provide whole-number ratings (e.g. 4.0) as opposed to fractional ratings (e.g. 3.5). ","metadata":{}},{"cell_type":"markdown","source":"Establish how many ratings each movie has received to get an indication of movie popularity.","metadata":{}},{"cell_type":"code","source":"movies_ranking['No_of_ratings'] = movies_metadata_df.groupby('title')['rating'].count()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:38:00.29056Z","iopub.execute_input":"2021-06-13T10:38:00.290865Z","iopub.status.idle":"2021-06-13T10:38:01.98338Z","shell.execute_reply.started":"2021-06-13T10:38:00.290835Z","shell.execute_reply":"2021-06-13T10:38:01.982274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies_ranking.sort_values(by=['No_of_ratings', 'rating'], ascending=False).head(15)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:38:01.98758Z","iopub.execute_input":"2021-06-13T10:38:01.987874Z","iopub.status.idle":"2021-06-13T10:38:02.015561Z","shell.execute_reply.started":"2021-06-13T10:38:01.987845Z","shell.execute_reply":"2021-06-13T10:38:02.014439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above table shows that the most famous movies of our time have received the highest number of ratings, giving an indicatio of the close connection between popularity and quality of movie.","metadata":{}},{"cell_type":"code","source":"# Set plot size\nsns.set(rc={'figure.figsize':(12,9)})\n\n# Plot Number of rating for every rating category.\nsns.scatterplot(x='rating', y='No_of_ratings', data=movies_ranking)\nplt.title('Number of ratings per average rating per movie')\nplt.xlabel('Rating')\nplt.ylabel('Number of ratings')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:38:02.019097Z","iopub.execute_input":"2021-06-13T10:38:02.019405Z","iopub.status.idle":"2021-06-13T10:38:02.383216Z","shell.execute_reply.started":"2021-06-13T10:38:02.019376Z","shell.execute_reply":"2021-06-13T10:38:02.382352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above plot shows that movies that receive a higher number of ratings also tend to receive higher rating scores, lending further credence to the belief that popular movies (high number of views) are more highly rated. In the plot below, it can be seen that movies with more than 100 ratings obtain a rating of 3.5 as their mode. ","metadata":{}},{"cell_type":"code","source":"# Average rating of movies in the dataset\navg_rating = train.groupby('movieId')['rating'].mean()\n\n# Plotting the results\nplt.figure(figsize=(12,10))\navg_rating.plot(kind='hist')\nplt.ylabel('Frequency')\nplt.xlabel('Movie Rating')\nplt.title('Average ratings of movies with 100 or more viewers')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:38:02.384353Z","iopub.execute_input":"2021-06-13T10:38:02.384635Z","iopub.status.idle":"2021-06-13T10:38:02.912018Z","shell.execute_reply.started":"2021-06-13T10:38:02.384607Z","shell.execute_reply":"2021-06-13T10:38:02.910969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is common knowledge that the identity of the director plays a large role in movie popularity and ratings. The table below shows the correlation between ratings count and rating (score) for each movie director. Those directors whose movies have received the most number of ratings typically also get higher rating scores for their movies.   ","metadata":{}},{"cell_type":"code","source":"best_director = pd.DataFrame(movies_metadata_df.groupby('director')['rating'].mean().\n                             sort_values(ascending=False))\nbest_director['No_of_ratings'] = movies_metadata_df.groupby('director')['rating'].count()\nbest_director.sort_values(by=['No_of_ratings', 'rating'], ascending=False).head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:38:02.913551Z","iopub.execute_input":"2021-06-13T10:38:02.913949Z","iopub.status.idle":"2021-06-13T10:38:05.435047Z","shell.execute_reply.started":"2021-06-13T10:38:02.913903Z","shell.execute_reply":"2021-06-13T10:38:05.434176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The scatterplot below provides a visual indication of the correlation between number of ratings and rating scores for directors. ","metadata":{}},{"cell_type":"code","source":"# Set plot size\nsns.set(rc={'figure.figsize':(12,9)})\n\nsns.scatterplot(x = 'rating', y = 'No_of_ratings', data = best_director).set_title('Number of ratings per average rating per director')\nplt.xlabel('Ratings')\nplt.ylabel('Number of Ratings')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:38:05.43678Z","iopub.execute_input":"2021-06-13T10:38:05.437083Z","iopub.status.idle":"2021-06-13T10:38:05.69378Z","shell.execute_reply.started":"2021-06-13T10:38:05.437054Z","shell.execute_reply":"2021-06-13T10:38:05.692519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Extracting insights from release dates","metadata":{}},{"cell_type":"code","source":"# Extract release date (year) from movie titles\n\n# Use regular expression to extract year from paranthesis in movie title \nmovies[\"year\"] = movies.title.str.extract(\"\\((\\d{4})\\)\", expand=True)\n# Remove parentheses so only year is extracted\nmovies[\"year\"] = movies.year.str.extract(\"(\\d\\d\\d\\d)\", expand=True)\n# Remove year from ‘title’ column\nmovies[\"title\"] = movies.title.str.replace(\"(\\(\\d\\d\\d\\d\\))\", \"\")\n# Strip any ending whitespace characters\nmovies[\"title\"] = movies[\"title\"].apply(lambda x: x.strip())","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:38:05.695217Z","iopub.execute_input":"2021-06-13T10:38:05.695548Z","iopub.status.idle":"2021-06-13T10:38:06.17885Z","shell.execute_reply.started":"2021-06-13T10:38:05.695519Z","shell.execute_reply":"2021-06-13T10:38:06.177955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove pipe character in genres column of movies dataset\nmovies['genres'] = movies['genres'].str.replace('|',' ')\n\n# Remove pipe character from title_cast column in imdb dataset\nimdb_data['title_cast'] = imdb_data['title_cast'].str.replace('|',' ')\nimdb_data['plot_keywords'] = imdb_data['plot_keywords'].str.replace('|',' ')","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:38:06.180009Z","iopub.execute_input":"2021-06-13T10:38:06.180471Z","iopub.status.idle":"2021-06-13T10:38:06.268135Z","shell.execute_reply.started":"2021-06-13T10:38:06.180437Z","shell.execute_reply":"2021-06-13T10:38:06.266695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imdb_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:38:06.269616Z","iopub.execute_input":"2021-06-13T10:38:06.269909Z","iopub.status.idle":"2021-06-13T10:38:06.284158Z","shell.execute_reply.started":"2021-06-13T10:38:06.26988Z","shell.execute_reply":"2021-06-13T10:38:06.283221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:38:06.285512Z","iopub.execute_input":"2021-06-13T10:38:06.28582Z","iopub.status.idle":"2021-06-13T10:38:06.308125Z","shell.execute_reply.started":"2021-06-13T10:38:06.285788Z","shell.execute_reply":"2021-06-13T10:38:06.306964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# map movie title to movieId:\nMapping_file = dict(zip(movies.title.tolist(), movies.movieId.tolist()))","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:38:06.309802Z","iopub.execute_input":"2021-06-13T10:38:06.310284Z","iopub.status.idle":"2021-06-13T10:38:06.337573Z","shell.execute_reply.started":"2021-06-13T10:38:06.31018Z","shell.execute_reply":"2021-06-13T10:38:06.336493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Consolidate movies and imdb dataframes \nmetadata = pd.merge(movies, imdb_data, on='movieId', how='left')\n# Calculate the number of movies released each year\nnum = metadata.groupby('year').count()\nplt.figure(figsize=(20,10))\nplt.plot(num.index, num['budget'])\nplt.xlabel(\"years\", size=25)\nplt.xticks(rotation='vertical')\nplt.ylabel('No. of Movies', size=25)\nplt.title('Number of Movies Released By year', size=25)\nplt.show()\n#metadata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:38:06.339068Z","iopub.execute_input":"2021-06-13T10:38:06.339392Z","iopub.status.idle":"2021-06-13T10:38:09.96873Z","shell.execute_reply.started":"2021-06-13T10:38:06.339363Z","shell.execute_reply":"2021-06-13T10:38:09.967575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To obtain further insight into the increasing number of movies since 2010, the wordplot below gives a breakdown of the most prominent years in this period.","metadata":{}},{"cell_type":"code","source":"# Calculate number of ratings by year \nyear_corpus = metadata['year'].value_counts()\n# Generating the wordcolud\nyear_wordcloud = WordCloud(background_color='white', height=2000, width=4000).generate_from_frequencies(year_corpus)\nplt.figure(figsize=(16,8))\nplt.imshow(year_wordcloud)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:38:09.970309Z","iopub.execute_input":"2021-06-13T10:38:09.97074Z","iopub.status.idle":"2021-06-13T10:38:30.529917Z","shell.execute_reply.started":"2021-06-13T10:38:09.970707Z","shell.execute_reply":"2021-06-13T10:38:30.528667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create 'year' df that averages release year for each user\nyears = pd.merge(train, movies, on='movieId')[['userId','year']].dropna()\nyears['year'] = years['year'].astype('int64')\nyears.groupby('userId').mean()\n\n# Set plot size\nsns.set(rc={'figure.figsize':(12,9)})\n\nplt.figure(figsize=(12,10))\nyears['year'].plot(kind='hist')\nplt.ylabel('Frequency')\nplt.xlabel('Average release year per user')\nplt.title('Distribution of release years for movies rated by users')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:38:30.531591Z","iopub.execute_input":"2021-06-13T10:38:30.532191Z","iopub.status.idle":"2021-06-13T10:38:38.107881Z","shell.execute_reply.started":"2021-06-13T10:38:30.532135Z","shell.execute_reply":"2021-06-13T10:38:38.107011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above plot shows how rating counts are distributed by decade with users showing a preference for movies from certain decades. Next we extract the year from the timestamp column within the train dataset in order to get an indication of the distribution of rating counts by year.","metadata":{}},{"cell_type":"code","source":"train['datetime_of_rating'] = train['timestamp'].apply(lambda x: datetime.fromtimestamp(x))\ntrain['year_rated'] = train['datetime_of_rating'].dt.year\ntrain_temp = train.drop(['timestamp', 'datetime_of_rating'], axis = 1)\ntrain_temp.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:38:38.108998Z","iopub.execute_input":"2021-06-13T10:38:38.109415Z","iopub.status.idle":"2021-06-13T10:38:54.618938Z","shell.execute_reply.started":"2021-06-13T10:38:38.109385Z","shell.execute_reply":"2021-06-13T10:38:54.618151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group ratings by year and derive an annual rating count\nyear_rated_df = train_temp[['year_rated','rating']].groupby('year_rated').count()\n\n# Set plot size\nsns.set(rc={'figure.figsize':(12,9)})\n\nsns.scatterplot(x = 'year_rated', y = 'rating', data = year_rated_df.reset_index()).set_title('Number of ratings per average rating per director')\nplt.xlabel('Year rated')\nplt.ylabel('number of ratings')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:38:54.620248Z","iopub.execute_input":"2021-06-13T10:38:54.620862Z","iopub.status.idle":"2021-06-13T10:38:55.160591Z","shell.execute_reply.started":"2021-06-13T10:38:54.620816Z","shell.execute_reply":"2021-06-13T10:38:55.15973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above plot shows that ratings were only collected from 1995 onwards. There is no clear correlation between rating counts and year in this time interval. The plot below uses a bar format to show the number of movie releases per year in descending order for the past 50 years.","metadata":{}},{"cell_type":"code","source":"#Plotting total amount of movies released in each year using a count plot.\nfigure= plt.subplots(figsize=(15, 5))\naxes=sns.countplot(x=movies['year'], order = movies['year'].value_counts()[0:50].index,color='blue')\naxes.set_title('Total movies released per year',fontsize=19)\nplt.xticks(rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:38:55.161883Z","iopub.execute_input":"2021-06-13T10:38:55.162481Z","iopub.status.idle":"2021-06-13T10:38:56.233877Z","shell.execute_reply.started":"2021-06-13T10:38:55.162436Z","shell.execute_reply":"2021-06-13T10:38:56.23279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above graph shows an increasing trend in movie releases since 1995. ","metadata":{}},{"cell_type":"markdown","source":"Let's now get an indication as to how rating counts vary by day of week","metadata":{}},{"cell_type":"code","source":"# Convert the timestamp values into datetime format\ntrain['timestamp'] = pd.to_datetime(train['timestamp'], unit='ms')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:38:56.235212Z","iopub.execute_input":"2021-06-13T10:38:56.235825Z","iopub.status.idle":"2021-06-13T10:38:56.556504Z","shell.execute_reply.started":"2021-06-13T10:38:56.235778Z","shell.execute_reply":"2021-06-13T10:38:56.555404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the day of the week from the newly-formatted timestamp column\ntrain['day_of_week'] = train['timestamp'].dt.dayofweek\ndays = {0:'Mon',1:'Tue',2:'Wed',3:'Thur',4:'Fri',5:'Sat',6:'Sun'}\ntrain['day_of_week'] = train['day_of_week'].apply(lambda x: days[x])\ntrain.tail()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:38:56.557794Z","iopub.execute_input":"2021-06-13T10:38:56.55839Z","iopub.status.idle":"2021-06-13T10:39:00.334772Z","shell.execute_reply.started":"2021-06-13T10:38:56.558344Z","shell.execute_reply":"2021-06-13T10:39:00.333618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot number of ratings by day of week\ntrain['day_of_week'].value_counts().plot(kind='bar')\nplt.title('Ratings per day of the week')\nplt.xlabel('Day of the week')\nplt.ylabel('Proportion of ratings created/collected')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:39:00.336204Z","iopub.execute_input":"2021-06-13T10:39:00.336533Z","iopub.status.idle":"2021-06-13T10:39:02.487695Z","shell.execute_reply.started":"2021-06-13T10:39:00.336503Z","shell.execute_reply":"2021-06-13T10:39:02.486455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is evident from the above that Saturdays and Sundays account for the highest number of ratings which can most likely be attributed to the fact that these are the days which people do not attend school, college, their place of work, etc.","metadata":{}},{"cell_type":"markdown","source":"#### Exploring Genres and Tags","metadata":{}},{"cell_type":"markdown","source":"Let's asses how prevalent each of the different genres are within our consolidated dataset","metadata":{}},{"cell_type":"code","source":"# Combine genres into single string\nmetadata['genres'] = metadata['genres'].astype('str')\ngenre_corpus = ' '.join(metadata['genres'])\n#Generate a list of stopwords to exclude from WordCloud\nstopword = ['no genres', 'no', 'genres', 'genre', 'listed']\n# Create the wordcolud\ngenre_wordcloud = WordCloud(stopwords=stopword, background_color='white', height=2000, width=4000).generate(genre_corpus)\nplt.figure(figsize=(16,8))\nplt.imshow(genre_wordcloud)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:39:02.489417Z","iopub.execute_input":"2021-06-13T10:39:02.489877Z","iopub.status.idle":"2021-06-13T10:39:24.272322Z","shell.execute_reply.started":"2021-06-13T10:39:02.489828Z","shell.execute_reply":"2021-06-13T10:39:24.271222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comedy, Drama, Sci-Fi and Romance constitute the most prevalent genres. Action, Adventure, Crime and Thriller also account for genres which are often watched.","metadata":{}},{"cell_type":"code","source":"sns.violinplot(x = 'runtime', data = metadata,).set_title('Distribution of Movie Duration')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:39:24.27375Z","iopub.execute_input":"2021-06-13T10:39:24.274054Z","iopub.status.idle":"2021-06-13T10:39:24.553703Z","shell.execute_reply.started":"2021-06-13T10:39:24.274025Z","shell.execute_reply":"2021-06-13T10:39:24.552765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('runtime mean: ', metadata['runtime'].mean())\nprint('runtime standard deviation: ', metadata['runtime'].std())","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:39:24.55502Z","iopub.execute_input":"2021-06-13T10:39:24.555552Z","iopub.status.idle":"2021-06-13T10:39:24.562776Z","shell.execute_reply.started":"2021-06-13T10:39:24.55551Z","shell.execute_reply":"2021-06-13T10:39:24.561415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The average movie runtime length is 101 minutes with the average deviation being approximately 30 minutes either side of this figure. Further analysis and modeling can reveal the extent to which runtime is a significant determinant of the rating given to a movie.","metadata":{}},{"cell_type":"markdown","source":"Movie tags are explored below. Each tag is given a relevance rating between 0 and 1 to indicate the extenmt to which the tag appropriately describes the movie.","metadata":{}},{"cell_type":"code","source":"genome_tags = pd.read_csv('/kaggle/input/edsa-movie-recommendation-challenge/genome_tags.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:39:24.5677Z","iopub.execute_input":"2021-06-13T10:39:24.568078Z","iopub.status.idle":"2021-06-13T10:39:24.582692Z","shell.execute_reply.started":"2021-06-13T10:39:24.568046Z","shell.execute_reply":"2021-06-13T10:39:24.581566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"genome_df = pd.merge(genome_scores,genome_tags, how='left',on='tagId')","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:39:24.584166Z","iopub.execute_input":"2021-06-13T10:39:24.584774Z","iopub.status.idle":"2021-06-13T10:39:26.869471Z","shell.execute_reply.started":"2021-06-13T10:39:24.584724Z","shell.execute_reply":"2021-06-13T10:39:26.868595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"genome_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:39:26.870686Z","iopub.execute_input":"2021-06-13T10:39:26.871169Z","iopub.status.idle":"2021-06-13T10:39:26.882209Z","shell.execute_reply.started":"2021-06-13T10:39:26.871118Z","shell.execute_reply":"2021-06-13T10:39:26.881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With most movies possessing multiple tags, let's sort each movie's tags in order of most to least relevant and establish how important a relevant tag is in determining a movie's rating.","metadata":{}},{"cell_type":"code","source":"genre_tag_df = genome_df.merge(movies, on = 'movieId', how = 'outer')","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:39:26.883967Z","iopub.execute_input":"2021-06-13T10:39:26.88451Z","iopub.status.idle":"2021-06-13T10:39:29.151676Z","shell.execute_reply.started":"2021-06-13T10:39:26.884474Z","shell.execute_reply":"2021-06-13T10:39:29.150596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"genre_tag_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:39:29.153022Z","iopub.execute_input":"2021-06-13T10:39:29.153345Z","iopub.status.idle":"2021-06-13T10:39:29.170108Z","shell.execute_reply.started":"2021-06-13T10:39:29.153296Z","shell.execute_reply":"2021-06-13T10:39:29.16882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"genre_tag_df.dropna(axis = 0, inplace = True) # Drop movies that don't have a tag\ngenre_tag_df['tag_rank'] = genre_tag_df.groupby(\"movieId\")[\"relevance\"].rank(method = \"first\", \n                                                                                       ascending = False).astype('int64')","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:39:29.171829Z","iopub.execute_input":"2021-06-13T10:39:29.172132Z","iopub.status.idle":"2021-06-13T10:39:43.859007Z","shell.execute_reply.started":"2021-06-13T10:39:29.172102Z","shell.execute_reply":"2021-06-13T10:39:43.857882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualise sorted tags for a selected movie ('Jumanji')\ngenre_tag_df[genre_tag_df.title == 'Jumanji'][['movieId','title','tag','relevance',\n                                                               'tag_rank']].sort_values(by = 'relevance'\n                                                                                              , ascending = False).head(10)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:39:43.860523Z","iopub.execute_input":"2021-06-13T10:39:43.860953Z","iopub.status.idle":"2021-06-13T10:39:45.936409Z","shell.execute_reply.started":"2021-06-13T10:39:43.860907Z","shell.execute_reply":"2021-06-13T10:39:45.935208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The tag that best defines the above movie is 'adventure'. There are several other tags also considered highly relevant, such as: 'children', 'fantasy' and 'jungle'. ","metadata":{}},{"cell_type":"markdown","source":"### Modeling","metadata":{}},{"cell_type":"markdown","source":"Having been given a choice to select either a content-based or collaborative-based algorithm, I have decided in favour of the latter. This technique is well-established, has been adopted by Netflix and performs well given a large database of users and a high number of historical ratings given to movies. Although it suffers from the 'cold-start' problem when a new user joins a movie-streaming service, this can be partly overcome by recommending popular movies. An advantage of the collaborative-based technique is the diversity of recommendations provided to users across a range of genres, actors, directors and release-year.   ","metadata":{}},{"cell_type":"code","source":"# Limit training sample to 500 000 rows \ntrain_sample = train.sample(n = 500000, replace = False)\n# Drop the timestamp column since it is not needed\ndata = train_sample.drop('timestamp', axis = 1)\n# Define the Reader object by specifying the rating scale range in the dataset\nreader = Reader(rating_scale=(0.5, 5), line_format='user item rating') \n# Load the dataset \ndata = Dataset.load_from_df(data[['userId', 'movieId', 'rating']], reader)\n# Split dataset into train and validation sets\ntrain_set, val_set = train_test_split(data, test_size=0.15, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:40:36.2484Z","iopub.execute_input":"2021-06-13T10:40:36.248837Z","iopub.status.idle":"2021-06-13T10:40:39.591561Z","shell.execute_reply.started":"2021-06-13T10:40:36.248803Z","shell.execute_reply":"2021-06-13T10:40:39.590354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Normal Predictor\nUnder the assumption of a normal distribution in the training set, this algorithm predicts a rating for each movie. \nAn RMSE of 1.45 is returned - not a great score but, at least, it's a start.","metadata":{}},{"cell_type":"code","source":"np_test = NormalPredictor()\nnp_test.fit(train_set)\npredictions = np_test.test(val_set)\n# Calculate RMSE\nnp_rmse = accuracy.rmse(predictions)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:40:39.593227Z","iopub.execute_input":"2021-06-13T10:40:39.593755Z","iopub.status.idle":"2021-06-13T10:40:41.942797Z","shell.execute_reply.started":"2021-06-13T10:40:39.593716Z","shell.execute_reply":"2021-06-13T10:40:41.941703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NMF\n\nNMF is a collaborative filtering algorithm whch makes use of non-negative matrix factorization to output predicted ratings for missing values. As seen below, it has improved out RMSE score to 1.07.","metadata":{}},{"cell_type":"code","source":"nmf = NMF()\nnmf.fit(train_set)\npredictions = nmf.test(val_set)\n# Calculate RMSE\nnmf_rmse = accuracy.rmse(predictions)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:40:41.946136Z","iopub.execute_input":"2021-06-13T10:40:41.946539Z","iopub.status.idle":"2021-06-13T10:41:48.187999Z","shell.execute_reply.started":"2021-06-13T10:40:41.946505Z","shell.execute_reply":"2021-06-13T10:41:48.18616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SlopeOne\n\nThe SlopeOne algorithm utilises linear regression to solve the data sparsity problem. Owing to the simplistic nature of the algorithm, we can see that the RMSE returned (1.16) is a long way off our target of 0.85.","metadata":{}},{"cell_type":"code","source":"slo = SlopeOne()\nslo.fit(train_set)\npredictions = slo.test(val_set)\n# Calculate RMSE\nslo_rmse = accuracy.rmse(predictions)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:41:48.190703Z","iopub.execute_input":"2021-06-13T10:41:48.191006Z","iopub.status.idle":"2021-06-13T10:42:06.47858Z","shell.execute_reply.started":"2021-06-13T10:41:48.190974Z","shell.execute_reply":"2021-06-13T10:42:06.477225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"BaselineOnly\n\nMaking use of either 'stochastic gradient descent' (SGD) or 'alternating least squares' (ALS), the Baseline Only algorithm predicts a baseline estimate rating for a given user and movie. With a returned RMSE score of 0.92, it looks like we're making good progress. ","metadata":{}},{"cell_type":"code","source":"bsl_choice= {'method': 'sgd','n_epochs': 40}\nblo = BaselineOnly(bsl_options=bsl_choice)\nblo.fit(train_set)\npredictions = blo.test(val_set)\n# Calculate RMSE\nblo_rmse = accuracy.rmse(predictions)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:42:06.479996Z","iopub.execute_input":"2021-06-13T10:42:06.4803Z","iopub.status.idle":"2021-06-13T10:42:15.489534Z","shell.execute_reply.started":"2021-06-13T10:42:06.48027Z","shell.execute_reply":"2021-06-13T10:42:15.488331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Co-Clustering\n\nIn similar manner to KMeans, the Co-clustering algorithm assigns data points to clusters using a straightforward optimization method. The RMSE achieved is 1.06.","metadata":{}},{"cell_type":"code","source":"coc= CoClustering(random_state=42)\ncoc.fit(train_set)\npredictions = coc.test(val_set)\n# Calculate RMSE\ncoc_rmse = accuracy.rmse(predictions)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:42:15.493088Z","iopub.execute_input":"2021-06-13T10:42:15.493423Z","iopub.status.idle":"2021-06-13T10:42:51.675822Z","shell.execute_reply.started":"2021-06-13T10:42:15.493385Z","shell.execute_reply":"2021-06-13T10:42:51.674636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Singular Value Decomposition (SVD)","metadata":{}},{"cell_type":"markdown","source":"The Singular Value Decomposition algorithm is a matrix factorization technique which reduces the number of features of a dataset. In the matrix structure, each row represents a user and each column represents a movie. The matrix elements are ratings that users give to movies. A basic SVD model with limit hyperparameter tuning returns a score of 0.92.","metadata":{}},{"cell_type":"code","source":"# Base algorithm\nalgo = SVD(n_epochs= 50, init_std_dev=0.02, n_factors=200)\n\n# Fit to trainset\nalgo.fit(train_set)\n \n# Make predictions on the validation dataset\npredictions_svd1 = algo.test(val_set)\nalgo_rmse = accuracy.rmse(predictions_svd1)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:42:51.676818Z","iopub.execute_input":"2021-06-13T10:42:51.677097Z","iopub.status.idle":"2021-06-13T10:46:06.580853Z","shell.execute_reply.started":"2021-06-13T10:42:51.677071Z","shell.execute_reply":"2021-06-13T10:46:06.579517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predictions_svd1","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:46:06.583718Z","iopub.execute_input":"2021-06-13T10:46:06.58403Z","iopub.status.idle":"2021-06-13T10:46:06.588647Z","shell.execute_reply.started":"2021-06-13T10:46:06.584Z","shell.execute_reply":"2021-06-13T10:46:06.587403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = pd.DataFrame(predictions_svd1)\npred","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:46:06.590089Z","iopub.execute_input":"2021-06-13T10:46:06.590448Z","iopub.status.idle":"2021-06-13T10:46:06.785729Z","shell.execute_reply.started":"2021-06-13T10:46:06.590407Z","shell.execute_reply":"2021-06-13T10:46:06.784378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred=pred.rename(columns={'uid':'userId', 'iid':'movieId','est':'rating'})\npred.drop(['r_ui','details'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:46:06.78715Z","iopub.execute_input":"2021-06-13T10:46:06.787512Z","iopub.status.idle":"2021-06-13T10:46:06.797512Z","shell.execute_reply.started":"2021-06-13T10:46:06.787478Z","shell.execute_reply":"2021-06-13T10:46:06.796479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:46:06.799075Z","iopub.execute_input":"2021-06-13T10:46:06.799413Z","iopub.status.idle":"2021-06-13T10:46:06.81274Z","shell.execute_reply.started":"2021-06-13T10:46:06.799376Z","shell.execute_reply":"2021-06-13T10:46:06.811438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nsvdpp_model = SVDpp(n_epochs=50,n_factors=400,init_std_dev=0.001,random_state=42, verbose=True)\nsvdpp_model.fit(trainset)\nsvdpp_predictions = svdpp_model.test(testset)\nsvdpp_rmse = accuracy.rmse(svdpp_predictions)\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svdpp_model = SVDpp(n_epochs=20,n_factors=400,init_std_dev=0.001,random_state=42, verbose=True)\nsvdpp_model.fit(train_set)\nsvdpp_predictions = svdpp_model.test(val_set)\nsvdpp_rmse = accuracy.rmse(svdpp_predictions)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:47:17.095072Z","iopub.execute_input":"2021-06-13T10:47:17.095491Z","iopub.status.idle":"2021-06-13T11:16:10.301541Z","shell.execute_reply.started":"2021-06-13T10:47:17.095458Z","shell.execute_reply":"2021-06-13T11:16:10.300392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n# Define search grid\nparam_grid = {'n_epochs': [40, 45,50], 'init_std_dev' : [0.02,0.05], 'n_factors' : [200,250]}\n\n# Instatiate gridsearch instance\ngs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=2)\n\n# Run gridsearch\ngs.fit(data)\n\n# best RMSE score\nprint(gs.best_score['rmse'])\n\n# combination of parameters that gave the best RMSE score\nprint(gs.best_params['rmse'])\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:28:33.733128Z","iopub.execute_input":"2021-06-13T10:28:33.733821Z","iopub.status.idle":"2021-06-13T10:28:33.74415Z","shell.execute_reply.started":"2021-06-13T10:28:33.733768Z","shell.execute_reply":"2021-06-13T10:28:33.742928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nsvdpp_model = SVDpp(n_epochs=50,n_factors=160,lr_all=0.0085,reg_all=0.02,init_std_dev=0.01,random_state=42, verbose=True)\nsvdpp_model.fit(train_set)\nsvdpp_predictions = svdpp_model.test(val_set)\nsvdpp_rmse = accuracy.rmse(svdpp_predictions)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-06-13T11:33:36.990301Z","iopub.execute_input":"2021-06-13T11:33:36.990739Z","iopub.status.idle":"2021-06-13T11:59:30.737363Z","shell.execute_reply.started":"2021-06-13T11:33:36.990704Z","shell.execute_reply":"2021-06-13T11:59:30.736384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rating_scale = Reader(rating_scale=(0.5, 5))\n#train_df = Dataset.load_from_df(train.drop('timestamp', axis=1), rating_scale)\ntrain_df = Dataset.load_from_df(train[['userId', 'movieId', 'rating']], rating_scale)\n\n# Training and validation set split for hypertuning\ntrain_set, val_set = train_test_split(train_df,\n                                      test_size=0.008,\n                                      random_state=42)\n\n# Modelling of the SVD hypertuning\nsvd_algo_hyper = SVD(n_factors=160, \n                     lr_all=0.0085,\n                     reg_all=0.02,\n                     n_epochs=20,\n                     init_std_dev=0.01)\nsvd_algo_hyper.fit(train_set)\n\n# Predicting on the validation set\nsvd_hyper_predictions = svd_algo_hyper.test(val_set)\n\n# Convert the predictions to dataframe\n#test = pd.DataFrame(predictions)\naccuracy.rmse(svd_hyper_predictions)\n\n# Dictionary for the data to log for the SVD tuned model\n#params = {'model_name': 'SVD_Tuned'}\n#metrics = {'RMSE': accuracy.rmse(svd_hyper_predictions)}\n\n# Log the parameters and results for the SVD tuned model\n#experiment.log_parameters(params)\n#experiment.log_parameters(metrics)\n# End the experiment for the SVD tuned experiment\n#experiment.end()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:15:17.529011Z","iopub.execute_input":"2021-06-13T10:15:17.529709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The RMSE of 0.93 achieved with this un-tuned SVD is not bad. It is not far off from the targeted RMSE value of 0.85. With some hyper-parameter tuning let's see how we can reduce this error metric further. ","metadata":{}},{"cell_type":"code","source":"\"\"\"\nrating_scale = Reader(rating_scale=(0.5, 5))\n#train_df = Dataset.load_from_df(train.drop('timestamp', axis=1), rating_scale)\ntrain_df = Dataset.load_from_df(train[['userId', 'movieId', 'rating']], rating_scale)\n\n# Training and validation set split for hypertuning\ntrain_set, val_set = train_test_split(train_df,\n                                      test_size=0.008,\n                                      random_state=42)\n\n# Modelling of the SVD hypertuning\nsvd_algo_hyper = SVD(n_factors=160, \n                     lr_all=0.0085,\n                     reg_all=0.02,\n                     n_epochs=20,\n                     init_std_dev=0.01)\nsvd_algo_hyper.fit(train_set)\n\n# Predicting on the validation set\nsvd_hyper_predictions = svd_algo_hyper.test(val_set)\n\n# Convert the predictions to dataframe\n#test = pd.DataFrame(predictions)\naccuracy.rmse(svd_hyper_predictions)\n\n# Dictionary for the data to log for the SVD tuned model\n#params = {'model_name': 'SVD_Tuned'}\n#metrics = {'RMSE': accuracy.rmse(svd_hyper_predictions)}\n\n# Log the parameters and results for the SVD tuned model\n#experiment.log_parameters(params)\n#experiment.log_parameters(metrics)\n# End the experiment for the SVD tuned experiment\n#experiment.end()\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-06-12T15:28:07.18459Z","iopub.execute_input":"2021-06-12T15:28:07.185125Z","iopub.status.idle":"2021-06-12T15:28:07.193885Z","shell.execute_reply.started":"2021-06-12T15:28:07.185075Z","shell.execute_reply":"2021-06-12T15:28:07.192526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our tuned algorithm gives an impressive RMSE score of 0.78. Let's make use of this tuned SVD algorithm to predict on our test set and see how it scores on the Kaggle public leaderboard. ","metadata":{}},{"cell_type":"code","source":"pred_svd_hyper = [svd_algo_hyper.predict(row.userId,\n                                         row.movieId) for idx,row in test.iterrows()]\n\n# Converting the predictions to a dataframe\ntest_pred_svd_hyper = pd.DataFrame(pred_svd_hyper)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T15:28:28.113594Z","iopub.execute_input":"2021-06-12T15:28:28.114007Z","iopub.status.idle":"2021-06-12T15:37:56.861232Z","shell.execute_reply.started":"2021-06-12T15:28:28.113958Z","shell.execute_reply":"2021-06-12T15:37:56.860028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submission dataset","metadata":{}},{"cell_type":"code","source":"# Rename the fields in the prediction dataframe\ntest_pred_svd_hyper.drop(['r_ui', 'details'], axis=1, inplace=True)\ntest_pred_svd_hyper = test_pred_svd_hyper.rename(columns={'uid':'userId',\n                                                          'iid':'movieId',\n                                                          'est':'rating'})\ntest_pred_svd_hyper.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T15:38:19.83802Z","iopub.execute_input":"2021-06-12T15:38:19.838466Z","iopub.status.idle":"2021-06-12T15:38:20.011167Z","shell.execute_reply.started":"2021-06-12T15:38:19.838428Z","shell.execute_reply":"2021-06-12T15:38:20.009842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate each userId and movieId into a single Id column for submission\ntest_pred_svd_hyper['Id'] =  test_pred_svd_hyper['userId'].astype(str).str.zfill(1) + '_' + test_pred_svd_hyper['movieId'].astype(str).str.zfill(1)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T15:38:21.72061Z","iopub.execute_input":"2021-06-12T15:38:21.721013Z","iopub.status.idle":"2021-06-12T15:38:41.4456Z","shell.execute_reply.started":"2021-06-12T15:38:21.720961Z","shell.execute_reply":"2021-06-12T15:38:41.444558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svd_hyper_predictions = test_pred_svd_hyper[['Id','rating']]\nsvd_hyper_predictions.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T15:38:41.447138Z","iopub.execute_input":"2021-06-12T15:38:41.44757Z","iopub.status.idle":"2021-06-12T15:38:41.596106Z","shell.execute_reply.started":"2021-06-12T15:38:41.447531Z","shell.execute_reply":"2021-06-12T15:38:41.594981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svd_hyper_predictions.to_csv('kaggle_submission7.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T15:38:41.597859Z","iopub.execute_input":"2021-06-12T15:38:41.598272Z","iopub.status.idle":"2021-06-12T15:38:59.506339Z","shell.execute_reply.started":"2021-06-12T15:38:41.598233Z","shell.execute_reply":"2021-06-12T15:38:59.50519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}